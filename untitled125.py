# -*- coding: utf-8 -*-
"""Untitled125.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bhc6IBjkbHzwRFMzQCjDRnIg7BIDQcmI
"""





!pip install PyPDF2 langchain faiss-cpu

!pip install -U langchain-community

import os
import torch
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from transformers import AutoModelForCausalLM, AutoTokenizer
from google.colab import drive

# تثبيت الاعتمادات اللازمة
!pip install PyPDF2 langchain faiss-cpu transformers torch

# تهيئة النماذج
def initialize_models():
    # تحميل نموذج التضمين من Hugging Face
    print("جاري تحميل نموذج التضمين...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # تحميل نموذج اللغة من Hugging Face
    print("جاري تحميل نموذج اللغة Phi-3.5-mini-instruct-exl2...")
    model_id = "bartowski/Phi-3.5-mini-instruct-exl2"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    return embedding_model, model, tokenizer

# استخراج النص من ملف PDF
def extract_text_from_pdf(pdf_path):
    print(f"جاري استخراج النص من الملف: {pdf_path}")
    text = ""
    try:
        pdf_reader = PdfReader(pdf_path)
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        print(f"تم استخراج {len(text)} حرف من النص.")
        return text
    except Exception as e:
        print(f"حدث خطأ أثناء قراءة ملف PDF: {e}")
        return None

# تقسيم النص إلى أجزاء صغيرة
def split_text(text):
    print("جاري تقسيم النص إلى أجزاء...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    print(f"تم تقسيم النص إلى {len(chunks)} جزء.")
    return chunks

# إنشاء قاعدة المعرفة (قاعدة البيانات المضمنة)
def create_knowledge_base(chunks, embedding_model):
    print("جاري إنشاء قاعدة المعرفة...")
    knowledge_base = FAISS.from_texts(chunks, embedding_model)
    print("تم إنشاء قاعدة المعرفة بنجاح.")
    return knowledge_base

# البحث عن المعلومات ذات الصلة
def search_knowledge_base(knowledge_base, query, k=4):
    print(f"جاري البحث عن: {query}")
    docs = knowledge_base.similarity_search(query, k=k)
    context = "\n\n".join(doc.page_content for doc in docs)
    return context

# توليد الإجابة باستخدام نموذج اللغة
def generate_answer(model, tokenizer, context, query):
    print("جاري توليد الإجابة...")

    prompt = f"""السياق:
{context}

السؤال:
{query}

الإجابة:
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=64,  # سياق بحجم 512
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        max_length=inputs.input_ids.shape[1] + 22  # توكن المخرجات 22
    )

    answer = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return answer

# الدالة الرئيسية
def main():
    print("برنامج RAG مع كتاب PDF")
    print("=" * 50)

    # الاتصال بجوجل درايف (اختياري)
    try:
        drive.mount('/content/drive')
        print("تم الاتصال بجوجل درايف بنجاح.")
    except:
        print("لم يتم الاتصال بجوجل درايف، سيتم استخدام المسارات المحلية.")

    # تهيئة النماذج
    embedding_model, llm_model, tokenizer = initialize_models()

    # طلب مسار الكتاب PDF من المستخدم
    pdf_path = input("أدخل مسار ملف PDF (على سبيل المثال: /content/drive/MyDrive/book.pdf): ")

    # استخراج النص وإنشاء قاعدة المعرفة
    text = extract_text_from_pdf(pdf_path)
    if text:
        chunks = split_text(text)
        knowledge_base = create_knowledge_base(chunks, embedding_model)

        # حلقة استعلام المستخدم
        while True:
            query = input("\nأدخل سؤالك (أو اكتب 'خروج' للخروج): ")
            if query.lower() == 'خروج' or query.lower() == 'exit':
                break

            # استرجاع السياق ذي الصلة
            context = search_knowledge_base(knowledge_base, query)

            # توليد الإجابة
            answer = generate_answer(llm_model, tokenizer, context, query)

            print("\nالإجابة:")
            print("-" * 50)
            print(answer)
            print("-" * 50)

    print("تم إنهاء البرنامج.")

if __name__ == "__main__":
    main()

import os
import subprocess
import torch
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# تثبيت الاعتمادات (إذا كنت تعمل في بيئة مثل Colab)
# !pip install -q PyPDF2 langchain faiss-cpu

# دالة لاستخراج النص من ملف PDF
def extract_text_from_pdf(pdf_path):
    print(f"جاري استخراج النص من الملف: {pdf_path}")
    text = ""
    try:
        pdf_reader = PdfReader(pdf_path)
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
        print(f"تم استخراج {len(text)} حرف من النص.")
        return text
    except Exception as e:
        print(f"حدث خطأ أثناء قراءة ملف PDF: {e}")
        return None

# دالة لتقسيم النص إلى أجزاء صغيرة
def split_text(text):
    print("جاري تقسيم النص إلى أجزاء...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    print(f"تم تقسيم النص إلى {len(chunks)} جزء.")
    return chunks

# إنشاء قاعدة المعرفة باستخدام FAISS
def create_knowledge_base(chunks, embedding_model):
    print("جاري إنشاء قاعدة المعرفة...")
    knowledge_base = FAISS.from_texts(chunks, embedding_model)
    print("تم إنشاء قاعدة المعرفة بنجاح.")
    return knowledge_base

# دالة لاسترجاع السياق من قاعدة المعرفة بناءً على استعلام المستخدم
def search_knowledge_base(knowledge_base, query, k=4):
    print(f"جاري البحث عن: {query}")
    docs = knowledge_base.similarity_search(query, k=k)
    context = "\n\n".join(doc.page_content for doc in docs)
    return context

# دالة لاستدعاء نموذج Exllama عبر subprocess
def generate_answer_exllama(model_path, prompt):
    """
    نفترض وجود سكريبت exllama_inference.py يقبل المعلمات:
    --model : مسار النموذج
    --prompt : النص الكامل (السياق والسؤال)
    ويمكنك تعديل الخيارات مثل max_new_tokens وtemperature حسب الحاجة.
    """
    command = [
        "python", "exllama_inference.py",
        "--model", model_path,
        "--prompt", prompt,
        # يمكنك إضافة معلمات أخرى مثل:
        # "--max_new_tokens", "64",
        # "--temperature", "0.7"
    ]

    try:
        # تشغيل الأمر واستلام النتيجة
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        output = result.stdout.strip()
        return output
    except subprocess.CalledProcessError as e:
        print("حدث خطأ أثناء استدعاء Exllama:", e)
        return None

def main():
    print("برنامج RAG مع كتاب PDF باستخدام نموذج Exllama")
    print("=" * 50)

    # خطوات إعداد قاعدة المعرفة:
    pdf_path = input("أدخل مسار ملف PDF (مثال: /content/drive/MyDrive/book.pdf): ").strip()
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return

    # استخدم نموذج التضمين المناسب (يمكنك استخدام أي نموذج مناسب مثل sentence-transformers)
    print("جاري تحميل نموذج التضمين...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    chunks = split_text(text)
    knowledge_base = create_knowledge_base(chunks, embedding_model)

    # مسار النموذج الخاص بـ Exllama (عدل المسار حسب مكان النموذج المحول)
    model_path = input("أدخل مسار نموذج Exllama (مثال: /path/to/exllama_model): ").strip()

    # حلقة استعلام المستخدم
    while True:
        query = input("\nأدخل سؤالك (أو اكتب 'خروج' للخروج): ").strip()
        if query.lower() in ['خروج', 'exit']:
            break

        # استرجاع السياق ذي الصلة
        context = search_knowledge_base(knowledge_base, query)

        # إنشاء prompt يحتوي على السياق والسؤال
        prompt = f"""السياق:
{context}

السؤال:
{query}

الإجابة:"""

        # استدعاء نموذج Exllama لتوليد الإجابة
        answer = generate_answer_exllama(model_path, prompt)

        print("\nالإجابة:")
        print("-" * 50)
        print(answer if answer else "لم يتم توليد إجابة.")
        print("-" * 50)

    print("تم إنهاء البرنامج.")

if __name__ == "__main__":
    main()

!git clone --single-branch --branch 3_5 https://huggingface.co/bartowski/Phi-3.5-mini-instruct-exl2

!rm -rf /content/Phi-3.5-mini-instruct-exl2

import os
import subprocess
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

def extract_text_from_pdf(pdf_path):
    print(f"جاري استخراج النص من الملف: {pdf_path}")
    text = ""
    try:
        pdf_reader = PdfReader(pdf_path)
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                # تنظيف بسيط للنص؛ يمكنك تعديل ذلك حسب الحاجة
                page_text = page_text.replace("\r", " ").replace("\t", " ")
                text += page_text + "\n"
        print(f"تم استخراج {len(text)} حرف من النص.")
        return text
    except Exception as e:
        print(f"حدث خطأ أثناء قراءة ملف PDF: {e}")
        return None

def split_text(text):
    print("جاري تقسيم النص إلى أجزاء...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
    chunks = splitter.split_text(text)
    print(f"تم تقسيم النص إلى {len(chunks)} جزء.")
    return chunks

def create_knowledge_base(chunks, embedding_model):
    print("جاري إنشاء قاعدة المعرفة...")
    kb = FAISS.from_texts(chunks, embedding_model)
    print("تم إنشاء قاعدة المعرفة بنجاح.")
    return kb

def search_knowledge_base(kb, query, k=4):
    print(f"جاري البحث عن: {query}")
    docs = kb.similarity_search(query, k=k)
    context = "\n\n".join(doc.page_content for doc in docs)
    return context

def generate_answer_exllama(model_path, prompt):
    """
    يقوم هذا الدالة باستدعاء سكريبت exllama_inference.py مع تمرير البرومبت والنموذج.
    يتم إضافة معلمات إضافية للتوليد (max_new_tokens, temperature, top_p).
    """
    command = [
        "python", "exllama_inference.py",
        "--model", model_path,
        "--prompt", prompt,
        "--max_new_tokens", "64",
        "--temperature", "0.7",
        "--top_p", "0.9"
    ]
    print("Executing command:", " ".join(command))
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        output = result.stdout.strip()
        return output
    except subprocess.CalledProcessError as e:
        print("حدث خطأ أثناء استدعاء Exllama:", e)
        print("stderr:", e.stderr)
        return None

def main():
    print("برنامج RAG مع كتاب PDF باستخدام نموذج Exllama")
    print("=" * 50)

    # طلب مسار ملف PDF
    pdf_path = input("أدخل مسار ملف PDF (مثال: /content/drive/MyDrive/book.pdf): ").strip()
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return

    # تقسيم النص وإنشاء قاعدة المعرفة
    chunks = split_text(text)

    print("جاري تحميل نموذج التضمين...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    kb = create_knowledge_base(chunks, embedding_model)

    # طلب مسار نموذج Exllama
    model_path = input("أدخل مسار نموذج Exllama (مثال: /content/Phi-3.5-mini-instruct-exl2): ").strip()

    # حلقة استعلام المستخدم
    while True:
        query = input("\nأدخل سؤالك (أو اكتب 'خروج' للخروج): ").strip()
        if query.lower() in ['خروج', 'exit']:
            break

        context = search_knowledge_base(kb, query)
        prompt = f"السياق:\n{context}\n\nالسؤال:\n{query}\n\nالإجابة:"

        answer = generate_answer_exllama(model_path, prompt)

        print("\nالإجابة:")
        print("-" * 50)
        if answer:
            print(answer)
        else:
            print("لم يتم توليد إجابة.")
        print("-" * 50)

    print("تم إنهاء البرنامج.")

if __name__ == "__main__":
    main()

import os
import subprocess
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

def extract_text_from_pdf(pdf_path):
    print(f"جاري استخراج النص من الملف: {pdf_path}")
    text = ""
    try:
        pdf_reader = PdfReader(pdf_path)
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                # تنظيف بسيط للنص؛ يمكنك تعديل ذلك حسب الحاجة
                page_text = page_text.replace("\r", " ").replace("\t", " ")
                text += page_text + "\n"
        print(f"تم استخراج {len(text)} حرف من النص.")
        return text
    except Exception as e:
        print(f"حدث خطأ أثناء قراءة ملف PDF: {e}")
        return None

def split_text(text):
    print("جاري تقسيم النص إلى أجزاء...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
    chunks = splitter.split_text(text)
    print(f"تم تقسيم النص إلى {len(chunks)} جزء.")
    return chunks

def create_knowledge_base(chunks, embedding_model):
    print("جاري إنشاء قاعدة المعرفة...")
    kb = FAISS.from_texts(chunks, embedding_model)
    print("تم إنشاء قاعدة المعرفة بنجاح.")
    return kb

def search_knowledge_base(kb, query, k=4):
    print(f"جاري البحث عن: {query}")
    docs = kb.similarity_search(query, k=k)
    context = "\n\n".join(doc.page_content for doc in docs)
    return context

def generate_answer_exllama(model_path, prompt):
    """
    يقوم هذا الدالة باستدعاء سكريبت exllama_inference.py مع تمرير البرومبت والنموذج.
    يتم إضافة معلمات إضافية للتوليد (max_new_tokens, temperature, top_p).
    """
    command = [
        "python", "exllama_inference.py",
        "--model", model_path,
        "--prompt", prompt,
        "--max_new_tokens", "64",
        "--temperature", "0.7",
        "--top_p", "0.9"
    ]
    print("Executing command:", " ".join(command))
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        output = result.stdout.strip()
        return output
    except subprocess.CalledProcessError as e:
        print("حدث خطأ أثناء استدعاء Exllama:", e)
        print("stderr:", e.stderr)
        return None

def main():
    print("برنامج RAG مع كتاب PDF باستخدام نموذج Exllama")
    print("=" * 50)

    # طلب مسار ملف PDF
    pdf_path = input("أدخل مسار ملف PDF (مثال: /content/drive/MyDrive/book.pdf): ").strip()
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return

    # تقسيم النص وإنشاء قاعدة المعرفة
    chunks = split_text(text)

    print("جاري تحميل نموذج التضمين...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    kb = create_knowledge_base(chunks, embedding_model)

    # طلب مسار نموذج Exllama
    model_path = input("أدخل مسار نموذج Exllama (مثال: /content/Phi-3.5-mini-instruct-exl2): ").strip()

    # حلقة استعلام المستخدم
    while True:
        query = input("\nأدخل سؤالك (أو اكتب 'خروج' للخروج): ").strip()
        if query.lower() in ['خروج', 'exit']:
            break

        context = search_knowledge_base(kb, query)
        prompt = f"السياق:\n{context}\n\nالسؤال:\n{query}\n\nالإجابة:"

        answer = generate_answer_exllama(model_path, prompt)

        print("\nالإجابة:")
        print("-" * 50)
        if answer:
            print(answer)
        else:
            print("لم يتم توليد إجابة.")
        print("-" * 50)

    print("تم إنهاء البرنامج.")

if __name__ == "__main__":
    main()

git clone https://github.com/turboderp/exllamav2
cd exllamav2
pip install -r requirements.txt
pip install .

python test_inference.py -m <path_to_model> -p "Once upon a time,"
# Append the '--gpu_split auto' flag for multi-GPU inference

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/turboderp/exllamav2
# %cd exllamav2
!pip install -r requirements.txt
!pip install .







import os
import subprocess
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

def extract_text_from_pdf(pdf_path):
    print(f"جاري استخراج النص من الملف: {pdf_path}")
    text = ""
    try:
        pdf_reader = PdfReader(pdf_path)
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                # تنظيف بسيط للنص؛ يمكنك تعديل ذلك حسب الحاجة
                page_text = page_text.replace("\r", " ").replace("\t", " ")
                text += page_text + "\n"
        print(f"تم استخراج {len(text)} حرف من النص.")
        return text
    except Exception as e:
        print(f"حدث خطأ أثناء قراءة ملف PDF: {e}")
        return None

def split_text(text):
    print("جاري تقسيم النص إلى أجزاء...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
    chunks = splitter.split_text(text)
    print(f"تم تقسيم النص إلى {len(chunks)} جزء.")
    return chunks

def create_knowledge_base(chunks, embedding_model):
    print("جاري إنشاء قاعدة المعرفة...")
    kb = FAISS.from_texts(chunks, embedding_model)
    print("تم إنشاء قاعدة المعرفة بنجاح.")
    return kb

def search_knowledge_base(kb, query, k=4):
    print(f"جاري البحث عن: {query}")
    docs = kb.similarity_search(query, k=k)
    context = "\n\n".join(doc.page_content for doc in docs)
    return context

def generate_answer_exllama(model_path, prompt):
    """
    يقوم هذا الدالة باستدعاء سكريبت exllama_inference.py مع تمرير البرومبت والنموذج.
    يتم إضافة معلمات إضافية للتوليد (max_new_tokens, temperature, top_p).
    """
    command = [
        "python", "exllama_inference.py",
        "--model", model_path,
        "--prompt", prompt,
        "--max_new_tokens", "64",
        "--temperature", "0.7",
        "--top_p", "0.9"
    ]
    print("Executing command:", " ".join(command))
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        output = result.stdout.strip()
        return output
    except subprocess.CalledProcessError as e:
        print("حدث خطأ أثناء استدعاء Exllama:", e)
        print("stderr:", e.stderr)
        return None

def main():
    print("برنامج RAG مع كتاب PDF باستخدام نموذج Exllama")
    print("=" * 50)

    # طلب مسار ملف PDF
    pdf_path = input("أدخل مسار ملف PDF (مثال: /content/drive/MyDrive/book.pdf): ").strip()
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return

    # تقسيم النص وإنشاء قاعدة المعرفة
    chunks = split_text(text)

    print("جاري تحميل نموذج التضمين...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    kb = create_knowledge_base(chunks, embedding_model)

    # طلب مسار نموذج Exllama
    model_path = input("أدخل مسار نموذج Exllama (مثال: /content/Phi-3.5-mini-instruct-exl2): ").strip()

    # حلقة استعلام المستخدم
    while True:
        query = input("\nأدخل سؤالك (أو اكتب 'خروج' للخروج): ").strip()
        if query.lower() in ['خروج', 'exit']:
            break

        context = search_knowledge_base(kb, query)
        prompt = f"السياق:\n{context}\n\nالسؤال:\n{query}\n\nالإجابة:"

        answer = generate_answer_exllama(model_path, prompt)

        print("\nالإجابة:")
        print("-" * 50)
        if answer:
            print(answer)
        else:
            print("لم يتم توليد إجابة.")
        print("-" * 50)

    print("تم إنهاء البرنامج.")

if __name__ == "__main__":
    main()

"""/content/exllamav2/test_inference.py

شغال
"""

import os
import subprocess
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

def extract_text_from_pdf(pdf_path):
    print(f"جاري استخراج النص من الملف: {pdf_path}")
    text = ""
    try:
        pdf_reader = PdfReader(pdf_path)
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                # تنظيف بسيط للنص؛ يمكنك تعديل ذلك حسب الحاجة
                page_text = page_text.replace("\r", " ").replace("\t", " ")
                text += page_text + "\n"
        print(f"تم استخراج {len(text)} حرف من النص.")
        return text
    except Exception as e:
        print(f"حدث خطأ أثناء قراءة ملف PDF: {e}")
        return None

def split_text(text):
    print("جاري تقسيم النص إلى أجزاء...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
    chunks = splitter.split_text(text)
    print(f"تم تقسيم النص إلى {len(chunks)} جزء.")
    return chunks

def create_knowledge_base(chunks, embedding_model):
    print("جاري إنشاء قاعدة المعرفة...")
    kb = FAISS.from_texts(chunks, embedding_model)
    print("تم إنشاء قاعدة المعرفة بنجاح.")
    return kb

def search_knowledge_base(kb, query, k=4):
    print(f"جاري البحث عن: {query}")
    docs = kb.similarity_search(query, k=k)
    context = "\n\n".join(doc.page_content for doc in docs)
    return context

def generate_answer_exllama(model_path, prompt):
    """
    يقوم هذا الدالة باستدعاء سكريبت الاستدلال الموجود في:
    /content/exllamav2/test_inference.py
    مع تمرير البرومبت والنموذج ومعلمات التوليد.
    """
    command = [
        "python", "/content/exllamav2/test_inference.py",
        "--model", model_path,
        "--prompt", prompt,
        "--max_new_tokens", "64",
        "--temperature", "0.7",
        "--top_p", "0.9"
    ]
    print("Executing command:", " ".join(command))
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        output = result.stdout.strip()
        return output
    except subprocess.CalledProcessError as e:
        print("حدث خطأ أثناء استدعاء Exllama:", e)
        print("stderr:", e.stderr)
        return None

def main():
    print("برنامج RAG مع كتاب PDF باستخدام نموذج Exllama")
    print("=" * 50)

    # طلب مسار ملف PDF
    pdf_path = input("أدخل مسار ملف PDF (مثال: /content/drive/MyDrive/book.pdf): ").strip()
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return

    # تقسيم النص وإنشاء قاعدة المعرفة
    chunks = split_text(text)

    print("جاري تحميل نموذج التضمين...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    kb = create_knowledge_base(chunks, embedding_model)

    # طلب مسار نموذج Exllama
    model_path = input("أدخل مسار نموذج Exllama (مثال: /content/Phi-3.5-mini-instruct-exl2): ").strip()

    # حلقة استعلام المستخدم
    while True:
        query = input("\nأدخل سؤالك (أو اكتب 'خروج' للخروج): ").strip()
        if query.lower() in ['خروج', 'exit']:
            break

        context = search_knowledge_base(kb, query)
        prompt = f"السياق:\n{context}\n\nالسؤال:\n{query}\n\nالإجابة:"

        answer = generate_answer_exllama(model_path, prompt)

        print("\nالإجابة:")
        print("-" * 50)
        if answer:
            print(answer)
        else:
            print("لم يتم توليد إجابة.")
        print("-" * 50)

    print("تم إنهاء البرنامج.")

if __name__ == "__main__":
    main()

import os
import subprocess
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

def extract_text_from_pdf(pdf_path):
    print(f"جاري استخراج النص من الملف: {pdf_path}")
    text = ""
    try:
        pdf_reader = PdfReader(pdf_path)
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                # تنظيف بسيط للنص؛ يمكنك تعديل ذلك حسب الحاجة
                page_text = page_text.replace("\r", " ").replace("\t", " ")
                text += page_text + "\n"
        print(f"تم استخراج {len(text)} حرف من النص.")
        return text
    except Exception as e:
        print(f"حدث خطأ أثناء قراءة ملف PDF: {e}")
        return None

def split_text(text):
    print("جاري تقسيم النص إلى أجزاء...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
    chunks = splitter.split_text(text)
    print(f"تم تقسيم النص إلى {len(chunks)} جزء.")
    return chunks

def create_knowledge_base(chunks, embedding_model):
    print("جاري إنشاء قاعدة المعرفة...")
    kb = FAISS.from_texts(chunks, embedding_model)
    print("تم إنشاء قاعدة المعرفة بنجاح.")
    return kb

def search_knowledge_base(kb, query, k=4):
    print(f"جاري البحث عن: {query}")
    docs = kb.similarity_search(query, k=k)
    context = "\n\n".join(doc.page_content for doc in docs)
    return context

def generate_answer_exllama(model_path, prompt):
    """
    يقوم هذا الدالة باستدعاء سكريبت الاستدلال الموجود في:
    /content/exllamav2/test_inference.py
    مع تمرير البرومبت والنموذج ومعلمات التوليد.
    """
    command = [
        "python", "/content/exllamav2/test_inference.py",
        "--model", model_path,
        "--prompt", prompt,
        "--max_new_tokens", "64",
        "--temperature", "0.7",
        "--top_p", "0.9"
    ]
    print("Executing command:", " ".join(command))
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        output = result.stdout.strip()
        return output
    except subprocess.CalledProcessError as e:
        print("حدث خطأ أثناء استدعاء Exllama:", e)
        print("stderr:", e.stderr)
        return None

def main():
    print("برنامج RAG مع كتاب PDF باستخدام نموذج Exllama")
    print("=" * 50)

    # طلب مسار ملف PDF
    pdf_path = input("أدخل مسار ملف PDF (مثال: /content/drive/MyDrive/book.pdf): ").strip()
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return

    # تقسيم النص وإنشاء قاعدة المعرفة
    chunks = split_text(text)

    print("جاري تحميل نموذج التضمين...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    kb = create_knowledge_base(chunks, embedding_model)

    # طلب مسار نموذج Exllama
    model_path = input("أدخل مسار نموذج Exllama (مثال: /content/Phi-3.5-mini-instruct-exl2): ").strip()

    # حلقة استعلام المستخدم
    while True:
        query = input("\nأدخل سؤالك (أو اكتب 'خروج' للخروج): ").strip()
        if query.lower() in ['خروج', 'exit']:
            break

        context = search_knowledge_base(kb, query)
        prompt = f"السياق:\n{context}\n\nالسؤال:\n{query}\n\nالإجابة:"

        answer = generate_answer_exllama(model_path, prompt)

        print("\nالإجابة:")
        print("-" * 50)
        if answer:
            print(answer)
        else:
            print("لم يتم توليد إجابة.")
        print("-" * 50)

    print("تم إنهاء البرنامج.")

if __name__ == "__main__":
    main()

import os
import subprocess
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

def extract_text_from_pdf(pdf_path):
    print(f"جاري استخراج النص من الملف: {pdf_path}")
    text = ""
    try:
        pdf_reader = PdfReader(pdf_path)
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                # تنظيف بسيط للنص؛ يمكنك تعديل ذلك حسب الحاجة
                page_text = page_text.replace("\r", " ").replace("\t", " ")
                text += page_text + "\n"
        print(f"تم استخراج {len(text)} حرف من النص.")
        return text
    except Exception as e:
        print(f"حدث خطأ أثناء قراءة ملف PDF: {e}")
        return None

def split_text(text):
    print("جاري تقسيم النص إلى أجزاء...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
    chunks = splitter.split_text(text)
    print(f"تم تقسيم النص إلى {len(chunks)} جزء.")
    return chunks

def create_knowledge_base(chunks, embedding_model):
    print("جاري إنشاء قاعدة المعرفة...")
    kb = FAISS.from_texts(chunks, embedding_model)
    print("تم إنشاء قاعدة المعرفة بنجاح.")
    return kb

def search_knowledge_base(kb, query, k=4):
    print(f"جاري البحث عن: {query}")
    docs = kb.similarity_search(query, k=k)
    context = "\n\n".join(doc.page_content for doc in docs)
    return context

def generate_answer_exllama(model_path, prompt):
    """
    يقوم هذا الدالة باستدعاء سكريبت test_inference.py الموجود في:
    /content/exllamav2/test_inference.py
    مع تمرير نص الاستعلام والنموذج ومعامل الحد الأقصى لطول الناتج.
    """
    command = [
        "python", "/content/exllamav2/test_inference.py",
        "-m", model_path,
        "-p", prompt,
        "-mol", "64"
    ]
    print("Executing command:", " ".join(command))
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        output = result.stdout.strip()
        return output
    except subprocess.CalledProcessError as e:
        print("حدث خطأ أثناء استدعاء Exllama:", e)
        print("stderr:", e.stderr)
        return None

def main():
    print("برنامج RAG مع كتاب PDF باستخدام نموذج Exllama")
    print("=" * 50)

    # طلب مسار ملف PDF
    pdf_path = input("أدخل مسار ملف PDF (مثال: /content/drive/MyDrive/book.pdf): ").strip()
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return

    # تقسيم النص وإنشاء قاعدة المعرفة
    chunks = split_text(text)

    print("جاري تحميل نموذج التضمين...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    kb = create_knowledge_base(chunks, embedding_model)

    # طلب مسار نموذج Exllama
    model_path = input("أدخل مسار نموذج Exllama (مثال: /content/Phi-3.5-mini-instruct-exl2): ").strip()

    # حلقة استعلام المستخدم
    while True:
        query = input("\nأدخل سؤالك (أو اكتب 'خروج' للخروج): ").strip()
        if query.lower() in ['خروج', 'exit']:
            break

        context = search_knowledge_base(kb, query)
        prompt = f"السياق:\n{context}\n\nالسؤال:\n{query}\n\nالإجابة:"

        answer = generate_answer_exllama(model_path, prompt)

        print("\nالإجابة:")
        print("-" * 50)
        if answer:
            print(answer)
        else:
            print("لم يتم توليد إجابة.")
        print("-" * 50)

    print("تم إنهاء البرنامج.")

if __name__ == "__main__":
    main()

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import os
import subprocess
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# تعيين متغير البيئة لتفادي تجزئة الذاكرة
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

def extract_text_from_pdf(pdf_path):
    print(f"جاري استخراج النص من الملف: {pdf_path}")
    text = ""
    try:
        pdf_reader = PdfReader(pdf_path)
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                # تنظيف بسيط للنص؛ يمكنك تعديل ذلك حسب الحاجة
                page_text = page_text.replace("\r", " ").replace("\t", " ")
                text += page_text + "\n"
        print(f"تم استخراج {len(text)} حرف من النص.")
        return text
    except Exception as e:
        print(f"حدث خطأ أثناء قراءة ملف PDF: {e}")
        return None

def split_text(text):
    print("جاري تقسيم النص إلى أجزاء...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)
    chunks = splitter.split_text(text)
    print(f"تم تقسيم النص إلى {len(chunks)} جزء.")
    return chunks

def create_knowledge_base(chunks, embedding_model):
    print("جاري إنشاء قاعدة المعرفة...")
    kb = FAISS.from_texts(chunks, embedding_model)
    print("تم إنشاء قاعدة المعرفة بنجاح.")
    return kb

def search_knowledge_base(kb, query, k=4):
    print(f"جاري البحث عن: {query}")
    docs = kb.similarity_search(query, k=k)
    context = "\n\n".join(doc.page_content for doc in docs)
    return context

def generate_answer_exllama(model_path, prompt):
    """
    يقوم هذا الدالة باستدعاء سكريبت test_inference.py الموجود في:
    /content/exllamav2/test_inference.py
    مع تمرير نص الاستعلام والنموذج ومعامل الحد الأقصى لطول الناتج.
    """
    command = [
        "python", "/content/exllamav2/test_inference.py",
        "-m", model_path,
        "-p", prompt,
        "-l", "32",
        "-mol", "32"
    ]
    print("Executing command:", " ".join(command))
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        output = result.stdout.strip()
        return output
    except subprocess.CalledProcessError as e:
        print("حدث خطأ أثناء استدعاء Exllama:", e)
        print("stderr:", e.stderr)
        return None

def main():
    print("برنامج RAG مع كتاب PDF باستخدام نموذج Exllama")
    print("=" * 50)

    # طلب مسار ملف PDF
    pdf_path = input("أدخل مسار ملف PDF (مثال: /content/drive/MyDrive/book.pdf): ").strip()
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return

    # تقسيم النص وإنشاء قاعدة المعرفة
    chunks = split_text(text)

    print("جاري تحميل نموذج التضمين...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    kb = create_knowledge_base(chunks, embedding_model)

    # طلب مسار نموذج Exllama
    model_path = input("أدخل مسار نموذج Exllama (مثال: /content/Phi-3.5-mini-instruct-exl2): ").strip()

    # حلقة استعلام المستخدم
    while True:
        query = input("\nأدخل سؤالك (أو اكتب 'خروج' للخروج): ").strip()
        if query.lower() in ['خروج', 'exit']:
            break

        context = search_knowledge_base(kb, query)
        prompt = f"السياق:\n{context}\n\nالسؤال:\n{query}\n\nالإجابة:"

        answer = generate_answer_exllama(model_path, prompt)

        print("\nالإجابة:")
        print("-" * 50)
        if answer:
            print(answer)
        else:
            print("لم يتم توليد إجابة.")
        print("-" * 50)

    print("تم إنهاء البرنامج.")

if __name__ == "__main__":
    main()

command = [
    "python", "/content/exllamav2/test_inference.py",
    "-m", model_path,
    "-p", prompt,
    "-l", "128",
    "-mol", "32"
]

!python /content/exllamav2/test_inference.py -http

!python /content/exllamav2/test_inference.py -http


usage: test_inference.py [-h] [-ed EVAL_DATASET] [-er EVAL_ROWS] [-el EVAL_LENGTH] [-et] [-e8]
                         [-eq4] [-eq6] [-eq8] [-ecl] [-p PROMPT] [-pnb] [-t TOKENS] [-ps] [-s]
                         [-mix MIX_LAYERS] [-nwu] [-sl] [-sp {wiki2}] [-rr RANK_REDUCE]
                         [-mol MAX_OUTPUT_LEN] [-m MODEL_DIR] [-gs GPU_SPLIT] [-tp] [-l LENGTH]
                         [-rs ROPE_SCALE] [-ra ROPE_ALPHA] [-ry ROPE_YARN] [-nfa] [-nxf] [-nsdpa]
                         [-ng] [-lm] [-ept EXPERTS_PER_TOKEN] [-lq4] [-fst] [-ic]
                         [-chunk CHUNK_SIZE]

Test inference on ExLlamaV2 model

options:
  -h, --help            show this help message and exit
  -ed EVAL_DATASET, --eval_dataset EVAL_DATASET
                        Perplexity evaluation dataset (.parquet file)
  -er EVAL_ROWS, --eval_rows EVAL_ROWS
                        Number of rows to apply from dataset (default 128)
  -el EVAL_LENGTH, --eval_length EVAL_LENGTH
                        Max no. tokens per sample
  -et, --eval_token     Evaluate perplexity on token-by-token inference using cache
  -e8, --eval_token_8bit
                        Evaluate perplexity on token-by-token inference using 8-bit (FP8) cache
  -eq4, --eval_token_q4
                        Evaluate perplexity on token-by-token inference using Q4 cache
  -eq6, --eval_token_q6
                        Evaluate perplexity on token-by-token inference using Q6 cache
  -eq8, --eval_token_q8
                        Evaluate perplexity on token-by-token inference using Q8 cache
  -ecl, --eval_context_lens
                        Evaluate perplexity at range of context lengths
  -p PROMPT, --prompt PROMPT
                        Generate from prompt (basic sampling settings)
  -pnb, --prompt_no_bos
                        Don't add BOS token to prompt
  -t TOKENS, --tokens TOKENS
                        Max no. tokens
  -ps, --prompt_speed   Test prompt processing (batch) speed over context length
  -s, --speed           Test raw generation speed over context length
  -mix MIX_LAYERS, --mix_layers MIX_LAYERS
                        Load replacement layers from secondary model. Example: --mix_layers
                        1,6-7:/mnt/models/other_model
  -nwu, --no_warmup     Skip warmup before testing model
  -sl, --stream_layers  Load model layer by layer (perplexity evaluation only)
  -sp {wiki2}, --standard_perplexity {wiki2}
                        Run standard (HF) perplexity test, stride 512 (experimental)
  -rr RANK_REDUCE, --rank_reduce RANK_REDUCE
                        Rank-reduction for MLP layers of model, in reverse order (for
                        experimentation)
  -mol MAX_OUTPUT_LEN, --max_output_len MAX_OUTPUT_LEN
                        Set max output chunk size (incompatible with ppl tests)
  -m MODEL_DIR, --model_dir MODEL_DIR
                        Path to model directory
  -gs GPU_SPLIT, --gpu_split GPU_SPLIT
                        "auto", or VRAM allocation per GPU in GB. "auto" is implied by default in
                        tensor-parallel mode.
  -tp, --tensor_parallel
                        Load in tensor-parallel mode (not fully supported for all models)
  -l LENGTH, --length LENGTH
                        Maximum sequence length
  -rs ROPE_SCALE, --rope_scale ROPE_SCALE
                        RoPE scaling factor
  -ra ROPE_ALPHA, --rope_alpha ROPE_ALPHA
                        RoPE alpha value (NTK)
  -ry ROPE_YARN, --rope_yarn ROPE_YARN
                        Set RoPE YaRN factor (use default max_seq_len as
                        original_max_position_embeddings if not configured)
  -nfa, --no_flash_attn
                        Disable Flash Attention
  -nxf, --no_xformers   Disable xformers, an alternative plan of flash attn for older devices
  -nsdpa, --no_sdpa     Disable Torch SDPA
  -ng, --no_graphs      Disable Graphs
  -lm, --low_mem        Enable VRAM optimizations, potentially trading off speed
  -ept EXPERTS_PER_TOKEN, --experts_per_token EXPERTS_PER_TOKEN
                        Override MoE model's default number of experts per token
  -lq4, --load_q4       Load weights in Q4 mode
  -fst, --fast_safetensors
                        Deprecated (does nothing)
  -ic, --ignore_compatibility
                        Do not override model config options in case of compatibility issues
  -chunk CHUNK_SIZE, --chunk_size CHUNK_SIZE
                        Chunk size ('input length')